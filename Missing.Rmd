---
title: "Missing Values"
author: "Divya Prima Crasta-237879"
date: "2024-11-22"
output: pdf_document
---

```{r}
library(Amelia)
library(miceRanger)

library(mlr3tuning)
library(mlr3learners)

library(mlr3extralearners)

library(dplyr)
library(ggplot2)
library(tidyr)
```


```{r}
X_n_l2 = X_n_l
X_n_l2[penalty.data[,-1] == 1 ] <-  NA
X_n_l2 = data.frame(X_n_l2)
data_n_l2 <- cbind(X_n_l2, Toxicity = as.factor(y))

# Define a classification task
task_wm <- TaskClassif$new(id = "data_wm", backend = data_n_l2, target = "Toxicity")
```


```{r}
missmap(X_n_l2)
nas = apply(X_n_l2, 2, function(col) sum(is.na(col)))
barplot(nas, las = 2)

# since ALOEC.Min, ALOEC.Med has no NA values, it need not be imputed, so we exclude it.
penalty_new <- penalty.data %>% select(-ALOEC.Min, -ALOEC.Med)

p_cor = apply(penalty_new[,-1], 2, function(col) cor(col, as.numeric(y)))
p_cor <- as.data.frame(sort(abs(round(p_cor,4)), decreasing = TRUE))
```

```{r}
penalty_cor <- cor(cbind(penalty.data[,-1], y = as.numeric(y)))
penalty_cor[,'y']
```

```{r}
# Convert y to numeric if necessary
y_numeric <- as.numeric(y)
#colors <- ifelse(y_numeric == 1, "red", "blue")

# Add jitter to y and plot
plot(
  penalty.data[,6], jitter(y_numeric),
  main = "Scatter Plot with Jitter",
  #xlab = "Penalty Data Column 2",
  ylab = "y (with jitter)",
  pch = 16
)

```


```{r}
penalty_vars <- penalty.data[, -1]

penalty.data$Toxicity <- assay.data$Toxicity

# Calculate the number of missing values (sum of 1s) for each variable grouped by Toxicity
missing_data <- penalty_vars %>%
  group_by(Toxicity = penalty.data$Toxicity) %>%
  summarise(across(everything(), sum, .names = "Missing_{.col}")) %>%
  pivot_longer(cols = -Toxicity, names_to = "Variable", values_to = "Missing") %>%
  mutate(Variable = gsub("Missing_", "", Variable))


# Create the barplot with rotated x-axis labels
ggplot(missing_data, aes(x = Variable, y = Missing, fill = as.factor(Toxicity))) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.9)) +
  scale_fill_manual(
    values = c("0" = "blue", "1" = "red"),
    labels = c("Toxicity = 0", "Toxicity = 1"),
    name = "Toxicity"
  ) +
  labs(
    title = "Number of Missing Values per Variable Grouped by Toxicity",
    x = "Variable",
    y = "Number of Missing Values"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))


```


# MICE - I

```{r}
mrModelOutput <- miceRanger(X_n_l2, valueSelector = "value",returnModels = TRUE, verbose=FALSE)

new_X <- impute(X_n_l2, mrModelOutput, verbose = FALSE)
dataList <- completeData(mrModelOutput)

task_m <- TaskClassif$new(id = "data", backend = cbind(Toxicity = as.factor(y), dataList[[1]]), target = "Toxicity")
```


## Random Forest

```{r}

lrn_rf<- lrn("classif.ranger",
  mtry = to_tune(1, 5, logscale = FALSE),
  num.trees = to_tune(100, 500, logscale = FALSE),
  min.bucket = to_tune(1, 5, logscale = FALSE),
  num.random.splits = to_tune(10, 50, logscale = FALSE)
 )

instance_rf_m <- ti(
  task = task_m,
  learner = lrn_rf,
  resampling = rsmp("cv", folds = 10),
  measures = msr("classif.ce"),
  terminator = trm("none")
)

tuner = tnr("grid_search", resolution = 5)

tuner$optimize(instance_rf_m)

instance_rf_m$result$learner_param_vals

print(instance_rf_m$result)

```

```{r}

lrn_gbm <- lrn("classif.gbm",
  n.trees = to_tune(100, 500, logscale = FALSE),
  interaction.depth = to_tune(1, 5, logscale = FALSE),
  shrinkage = to_tune(0.01, 0.05, logscale = FALSE)
 )

instance_gbm_m <- ti(
  task = task_m,
  learner = lrn_gbm,
  resampling = rsmp("cv", folds = 10),
  measures = msr("classif.ce"),
  terminator = trm("none")
)

tuner = tnr("grid_search", resolution = 5)

tuner$optimize(instance_gbm_m)

#instance_rf_uni$result$learner_param_vals

print(instance_gbm_m$result)
```

```{r}
lrn_xgb <- lrn("classif.xgboost",
  nrounds = to_tune(100, 500, logscale = FALSE),
  max_depth = to_tune(1, 5, logscale = FALSE),
  eta = to_tune(0.1, 0.5, logscale = FALSE)
 )

instance_xgb_m <- ti(
  task = task_m,
  learner = lrn_xgb,
  resampling = rsmp("cv", folds = 10),
  measures = msr("classif.ce"),
  terminator = trm("none")
)

tuner = tnr("grid_search", resolution = 5)

tuner$optimize(instance_xgb_m)

#instance_rf_uni$result$learner_param_vals

print(instance_xgb_m$result)
```

# MICE - II

```{r}
mrMeanMatch <- miceRanger(X_n_l2, valueSelector = "meanMatch", returnModels = TRUE, verbose=FALSE)

new_X2 <- impute(X_n_l2, mrMeanMatch, verbose = FALSE)
dataList2 <- completeData(mrMeanMatch)

task_m2 <- TaskClassif$new(id = "data", backend = cbind(Toxicity = as.factor(y), dataList2[[1]]), target = "Toxicity")
```

## Random Forest
```{r}

lrn_rf<- lrn("classif.ranger",
  mtry = to_tune(1, 5, logscale = FALSE),
  num.trees = to_tune(100, 500, logscale = FALSE),
  min.bucket = to_tune(1, 5, logscale = FALSE),
  num.random.splits = to_tune(10, 50, logscale = FALSE)
 )

instance_rf_m2 <- ti(
  task = task_m2,
  learner = lrn_rf,
  resampling = rsmp("cv", folds = 10),
  measures = msr("classif.ce"),
  terminator = trm("none")
)

tuner = tnr("grid_search", resolution = 5)

tuner$optimize(instance_rf_m2)

instance_rf_m2$result$learner_param_vals

print(instance_rf_m2$result)

```


```{r}
lrn_gbm <- lrn("classif.gbm",
  n.trees = to_tune(100, 500, logscale = FALSE),
  interaction.depth = to_tune(1, 5, logscale = FALSE),
  shrinkage = to_tune(0.01, 0.05, logscale = FALSE)
 )

instance_gbm_m2 <- ti(
  task = task_m2,
  learner = lrn_gbm,
  resampling = rsmp("cv", folds = 10),
  measures = msr("classif.ce"),
  terminator = trm("none")
)

tuner = tnr("grid_search", resolution = 5)

tuner$optimize(instance_gbm_m2)

#instance_rf_uni$result$learner_param_vals

print(instance_gbm_m2$result)
```

```{r}
lrn_xgb <- lrn("classif.xgboost",
  nrounds = to_tune(100, 500, logscale = FALSE),
  max_depth = to_tune(1, 5, logscale = FALSE),
  eta = to_tune(0.1, 0.5, logscale = FALSE)
 )

instance_xgb_m2 <- ti(
  task = task_m2,
  learner = lrn_xgb,
  resampling = rsmp("cv", folds = 10),
  measures = msr("classif.ce"),
  terminator = trm("none")
)

tuner = tnr("grid_search", resolution = 5)

tuner$optimize(instance_xgb_m2)

#instance_rf_uni$result$learner_param_vals

print(instance_xgb_m2$result)
```


# Miss Forest
```{r}
library(missForest)
librar <- (PRROC)
train_index <- sample(nrow(data_n_l), 80) 
y <- as.factor(y)

#train <- data_n_l[train_index, ]
#test <- data_n_l[-train_index, ]

train_X <- X_n_l2[train_index, ]
test_X <- X_n_l2[-train_index, ]

# 1) impute train
imp_train_X <- missForest(train_X)$ximp
train_test_X <- rbind(test_X, imp_train_X)
imp_test_X <- missForest(train_test_X)$ximp[1:nrow(test_X), ]

# 2) & 3) combine & impute test
train_test_X <- rbind(test_X, imp_train_X)
imp_test_X <- missForest(train_test_X)$ximp[1:nrow(test_X), ]

library(randomForest)
# 4) build model
rf <- randomForest(x = imp_train_X, y = y[train_index], mtry = 1, ntree = 200, sampsize = 3)

#----------tuning here?--------------------


# 5) predict for test
pred_test <- predict(rf, imp_test_X, type = "prob")

# 6) test ROC & AUC
test_scores <- data.frame(event_prob = pred_test[ ,2], labels = y[-train_index])

test_roc_v1 <- roc.curve(scores.class0 = test_scores[test_scores$labels == 1, ]$event_prob, # scores for the POSITIVE class
                      scores.class1 = test_scores[test_scores$labels == 0, ]$event_prob, # scores for the NEGATIVE class
                      curve=T)
test_roc_v1$auc

pred <- ifelse(pred_test[ , 2]<0.5, 0, 1)

sum(pred == y[-train_index])/20

#--------- iterate this process for every fold in cross fold (with stratification?) ? -------
```



```{r}
# 1) impute train
imp_X <- missForest(as.data.frame(X_n_l2))$ximp

# 2) build model
task_miss <- TaskClassif$new(id = "data", backend = cbind(imp_X, Toxicity = as.factor(y)), target = "Toxicity")

```

## Random Forest

```{r}
lrn_rf <- lrn("classif.ranger",
  mtry = to_tune(1, 5, logscale = FALSE),
  num.trees = to_tune(100, 500, logscale = FALSE),
  min.bucket = to_tune(1, 5, logscale = FALSE),
  num.random.splits = to_tune(10, 50, logscale = FALSE)
 )

instance_rf_miss <- ti(
  task = task_miss,
  learner = lrn_rf,
  resampling = rsmp("cv", folds = 10),
  measures = msr("classif.ce"),
  terminator = trm("none")
)

tuner = tnr("grid_search", resolution = 5)

tuner$optimize(instance_rf_miss)

instance_rf_miss$result$learner_param_vals

print(instance_rf_miss$result)

```

## Gradient Boosting

```{r}
lrn_gbm <- lrn("classif.gbm",
  n.trees = to_tune(100, 500, logscale = FALSE),
  interaction.depth = to_tune(1, 5, logscale = FALSE),
  shrinkage = to_tune(0.01, 0.05, logscale = FALSE)
 )

instance_gbm_miss <- ti(
  task = task_miss,
  learner = lrn_gbm,
  resampling = rsmp("cv", folds = 10),
  measures = msr("classif.ce"),
  terminator = trm("none")
)

tuner = tnr("grid_search", resolution = 5)

tuner$optimize(instance_gbm_miss)

#instance_rf_uni$result$learner_param_vals

print(instance_gbm_miss$result)
```

## XGBoost

```{r}
lrn_xgb <- lrn("classif.xgboost",
  nrounds = to_tune(100, 500, logscale = FALSE),
  max_depth = to_tune(1, 5, logscale = FALSE),
  eta = to_tune(0.1, 0.5, logscale = FALSE)
 )

instance_xgb_miss <- ti(
  task = task_miss,
  learner = lrn_xgb,
  resampling = rsmp("cv", folds = 10),
  measures = msr("classif.ce"),
  terminator = trm("none")
)

tuner = tnr("grid_search", resolution = 5)

tuner$optimize(instance_xgb_miss)

#instance_rf_uni$result$learner_param_vals

print(instance_xgb_miss$result)
```



# Handling missing values in rf

## imputation

### Random Forest

```{r}
data_imp_rf <- rfImpute(Toxicity~., y = y, cbind(X_n_l2, Toxicity = y))
#----------tuning for rfImpute?

#----------------tune this?

task_m_rf <- TaskClassif$new(id = "data", backend = data_imp_rf, target = "Toxicity")

lrn_rf <- lrn("classif.ranger",
  mtry = to_tune(1, 5, logscale = FALSE),
  num.trees = to_tune(100, 500, logscale = FALSE),
  min.bucket = to_tune(1, 5, logscale = FALSE),
  num.random.splits = to_tune(10, 50, logscale = FALSE)
 )

instance_m_rf <- ti(
  task = task_m_rf,
  learner = lrn_rf,
  resampling = rsmp("cv", folds = 10),
  measures = msr("classif.ce"),
  terminator = trm("none")
)

tuner = tnr("grid_search", resolution = 5)

tuner$optimize(instance_m_rf)

instance_m_rf$result$learner_param_vals

print(instance_m_rf$result)

```
### Gradient Boost

```{r}
lrn_gbm <- lrn("classif.gbm",
  n.trees = to_tune(100, 500, logscale = FALSE),
  interaction.depth = to_tune(1, 5, logscale = FALSE),
  shrinkage = to_tune(0.01, 0.05, logscale = FALSE)
 )

instance_gbm_m_rf <- ti(
  task = task_m_rf,
  learner = lrn_gbm,
  resampling = rsmp("cv", folds = 10),
  measures = msr("classif.ce"),
  terminator = trm("none")
)

tuner = tnr("grid_search", resolution = 5)

tuner$optimize(instance_gbm_m_rf)

#instance_rf_uni$result$learner_param_vals

print(instance_gbm_m_rf$result)
```



### XGBoost

```{r}
lrn_xgb <- lrn("classif.xgboost",
  nrounds = to_tune(100, 500, logscale = FALSE),
  max_depth = to_tune(1, 5, logscale = FALSE),
  eta = to_tune(0.1, 0.5, logscale = FALSE)
 )

instance_xgb_m_rf <- ti(
  task = task_m_rf,
  learner = lrn_xgb,
  resampling = rsmp("cv", folds = 10),
  measures = msr("classif.ce"),
  terminator = trm("none")
)

tuner = tnr("grid_search", resolution = 5)

tuner$optimize(instance_xgb_m_rf)

#instance_rf_uni$result$learner_param_vals

print(instance_xgb_m_rf$result)
```

## Surrogate

```{r}
library(party)
crf <- cforest(Toxicity~., data = cbind(X_n_l2, Toxicity = y))
#print(sum(predict(crf) == y)/100)

#cv(cforest, Toxicity~. , data = cbind(X_n_l2, Toxicity = y), k = 10, predict.fun = predictCRF)

tune_crf <- my_tune(cforest, Toxicity~. , data_n_l2, k = 10, run = run_crf, predict.fun = predictCRF, 
                    mtry = c(1, 2, 3, 4, 5),
                    ntree = c(100, 200, 300, 400, 500)#,
                    #mincriterion = c(0.1, 0.2, 0.3, 0.4, 0.5)
                    )

tune_crf[[2]]
```


```{r}
library(mlr3)
library(party)

# Define learner
lrn_rf_rf <- lrn("classif.cforest",
               mtry = to_tune(1, 5, logscale = FALSE),
               mincriterion = to_tune(0.01, 0.55, logscale = FALSE))

instance_rf_rf <- ti(
  task = task_wm,
  learner = lrn_rf_rf,
  resampling = rsmp("cv", folds = 10),
  measures = msr("classif.ce"),
  terminator = trm("none")
)

tuner = tnr("grid_search", resolution = 5)

tuner$optimize(instance_rf_rf)

# Best hyperparameters
instance_rf_rf$result

```

